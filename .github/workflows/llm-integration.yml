name: LLM Integration Tests

on:
  push:
    branches:
      - main
      - master

jobs:
  llm-integration-test:
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: pip install -r backend/requirements.txt

      - name: Install and start Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh
          ollama serve &
          for i in $(seq 1 30); do
            curl -sf http://localhost:11434/api/tags > /dev/null && break
            sleep 1
          done

      - name: Pull model
        run: ollama pull qwen2.5:0.5b

      - name: Verify Ollama is running
        run: |
          curl -f http://localhost:11434/api/tags
          ollama list

      - name: Warm up model
        run: |
          curl -s http://localhost:11434/api/generate -d '{"model":"qwen2.5:0.5b","prompt":"hi","stream":false}' --max-time 120 | head -c 200
          echo ""

      - name: Run LLM integration tests
        env:
          OLLAMA_URL: http://localhost:11434
          OLLAMA_MODEL: qwen2.5:0.5b
        run: |
          pytest backend/tests/test_llm_integration.py -v -m integration --timeout=300

      - name: Run vision detection integration tests
        env:
          OLLAMA_URL: http://localhost:11434
          OLLAMA_MODEL: qwen2.5:0.5b
        run: |
          pytest backend/tests/test_vision_detection_integration.py -v -m integration --timeout=300
